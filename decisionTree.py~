# decisionTree.py
# -----------------------
# COSC480 AI, Colgate University

import classificationMethod
import math
import util

class DecisionTreeClassifer(classificationMethod.ClassificationMethod):
    '''
    Decision tree classifer.

    Note that the variable 'datum' in this code refers to a counter of features
    (not to raw data such as that produced by the loadXXX functions in samples.py).
    '''

    def __init__(self, legalLabels, max_depth=30):  # feel free to change default max_depth
        self.legalLabels = legalLabels
        self.depth = max_depth 
        self.root = None       # training should replace this with root of decision tree!
        "*** YOUR CODE HERE ***"
        self.values = {}

    def train(self, trainingData, trainingLabels):
        '''
        Train the decision tree on training data.
        '''
        # might be useful in your code later...
        # this is a list of all features in the training set.
        features = list(set([ f for datum in trainingData for f in datum.keys() ]));
        "*** YOUR CODE HERE ***"
        for feature in features:
            self.values[feature]=[]
            counter = util.Counter()
            for i in range(len(trainingData)):
                counter[trainingData[i][feature]]+=1
            for key in counter.keys():
                self.values[feature].append(key)
        root = self.learn(trainingData,trainingLabels,features,None,None,None)
        self.root = root

    def getPlularity(self,examples,exampleLabels):
        counter = util.Counter()
        for i in range(len(examples)):
            counter[exampleLabels[i]]+=1
        return counter.argMax()

    def learn(self, examples,labels, features, parent_examples,parent_labels, parent):
        if not examples:
            plularity=self.getPlularity(parent_examples,parent_labels)
            node = DecisionNode(parent,{},True,None,plularity,0.0)
            return node
        if not features:
            plularity = self.getPlularity(examples,labels)
            node = DecisionNode(parent,{},True,None,plularity,0.0)
            return node
        else:
            counter = util.Counter()
            for i in range(len(examples)):
                counter[labels[i]]+=1
            if len(counter.keys())==1:
                node = DecisionNode(parent,{},True,None,counter.argMax(),0.0)
                return node
            gain = 0
            splitFeature = features[0]
            entropy = 0
            for key in counter.keys():
                temp = float(counter[key])/len(examples)
                entropy = entropy + temp*math.log(temp,2)
            entropy = entropy*(-1)
            print entropy
            for feature in features:
                counter = util.Counter()
                for i in range(len(examples)):
                    counter[examples[i][feature]]+=1
                keyEn = 0
                for key1 in counter.keys():
                    split = util.Counter()
                    for i in range(len(examples)):
                        if examples[i][feature]==key1:
                            split[labels[i]]+=1
                    tempEntropy = 0
                    for key2 in split.keys():
                        temp = float(split[key2])/(counter[key1])
                        tempEntropy += temp*math.log(temp,2)
                    tempEntropy = tempEntropy*(-1)
                    keyEn += float(counter[key1])/len(examples)*tempEntropy
                if gain<(entropy-keyEn):
                    gain = entropy-keyEn
                    splitFeature = feature
                    values = counter.keys()
            node = DecisionNode(parent,{},False,splitFeature,None,gain)
            for value in self.values[splitFeature]:
                nextExamples = []
                nextLabels = []
                newFeatures = features[:]
                newFeatures.remove(splitFeature)
                for i in range(len(examples)):
                    if examples[i][splitFeature]==value:
                        nextExamples.append(examples[i])
                        nextLabels.append(labels[i])
                node.child[value]= self.learn(nextExamples,nextLabels,newFeatures,examples,labels,node)
            return node


    def classify(self, data):
        """
        Classifies each datum by passing it through the decision tree.  

        Expects data to be a list.  Each datum in list is a feature vector (i.e., a dictionary)
        """
        "*** YOUR CODE HERE ***"

        guesses = []
        for datum in data:
            currentNode = self.root
            while not currentNode.leaf:
                feature = currentNode.attr
                value = datum[feature]
                #print currentNode.attr,value
                currentNode = currentNode.child[value]
            guesses.append(currentNode.label)
        return guesses


    def printDiagnostics(self):
        """
        This function is called after the classifier has been trained.  

        It should print the first five levels of the decision tree in a nicely 
        formatted way.  
        """
        "*** YOUR CODE HERE ***"
        def printhelper(node,depth):
            if depth>=5:
                return
            else:
                print "\t"*depth+"Split on "+node.attr+" gain="+str(node.gain)
                for key in node.child:
                    if node.child[key].leaf:
                        print "\t"*(depth+1)+"value="+key+" label="+node.child[key].label
                    else:
                        printhelper(node.child[key],depth+1)


        current = self.root
        printhelper(current,0)

class DecisionNode:
    def __init__(self, parent, child, leaf, attr,label,gain):
        self.parent = parent
        self.child = child
        self.leaf = leaf
        self.attr = attr
        self.label = label
        self.gain = gain
                                
"*** YOUR CODE HERE ***"
"""
You may find it helpful to create additional classes/functions
such as making a DecisionNode class that represents a single 
node in the decision tree.
"""

